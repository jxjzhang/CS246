Squeeze
In the twitter environment, people may repeat one character in some words for many times to express their emotions or to emphasis such words. For example, people may say:"I am soooooooooooo happy!". In this case, the words 'so' is stretched by repeating the character 'o' for many times. In order to make the content more clear, we are trying to recover the original words.
 
An important fact in English is that no words contain three same consecutive characters. Based on this fact, we designed our squeezing algorithm. For each words, we will scan this word and check whether it has three or more same adjacent characters. If so, our algorithm will squeeze these characters and only keep one or two characters. For example, the word "goooooooooood" will be squeezed to "good" and god. More over, if we find multiple such "squeezable" sections in a words, we will squeeze them separately and return all possible squeeze result. For example, if we have a word "bammmmmmmmboooooooo", the final squeeze result will be "bambo", "bamboo", "bammbo", "bammboo".

Edit distance
Edit distance is a way to quantifying how dissimilar two words are to one another by counting the minimum number of operations required to transform one string into the other. Using different set of operations can define different edit distances. In this project, we use the Damerau-Levenshtein distance. The operation for this metric is defined as insertion, deletion, substitution of a single word and transposition of two adjacent characters. 
We use the edit distance to generate the candidate list for each words. We generate the candidate list by maintain a distance-based candidate list and enlarge such candidate list in a iterative way. For example, we first generate the candidate with the edit distance one by applying insertion, deletion, substitution and transposition. We call this set S1. Then for each words in S1, we can generate a new set S2 by applying the same operations on the set S1. In this way, we can get candidate set with fixed distance to the original words. Moreover, in each step, in order to avoid generate too much candidate, we use a dictionary to filter out all the words which is not existed in the candidate list. In this project, we limit the maximum distance as 2 to generate the candidate list.

Bigram correction
When we finished the single words correction, each word will have a potential candidate list. However, we don't know which one is more proper. So we use an bigram correction to make the final choice. This algorithm works in the following way. 
Assuming we have two adjacent words w1 and w2 and they have the corresponding candidate list p and q. We also assume p[i] is the i-th candidate words in the list p and q[j] means the j-th candidate in q. Then for each candidate pair (p[i], q[j]), we will call it a bigram unit. For each bigram unit, we will check it's frequency in our bigram frequency dictionary. The frequency of the bigram unit is the score of this bigram unit. If the bigram unit is (p[i],q[j]), we note this score as F(p[i],q[j]). For each candidate, we also have a similarity score to the original words. If we assume the candidate is p[i], the similarity score is noted as S(p[i]). For each word, we noted the candidate list of the previous word as p, the candidate list of itself as q, the candidate list of the word after it as r. We have the formula below to calculate the final score of each candidate qj].
R(q[j]) = max(F(p[i],q[j]))*max(F(q[j],r[k]))*S(q[j])
Finally, among all the R(q[j]), we will pick up the maximum on as the final result of q.
For example, we have three candidate lists, (a), (good, god), (boy, buy). Assuming the bigram scores are F(a,good)=10, F(a, god)=10, F(good, boy)=100, F(god, boy)=1, F(good, buy)=5, F(god, buy)=1 and the similarity score is S(good)=10, S(god)=3. So the final score R(good)=F(a,good)*F(good,boy)*S(good)=10*100*10=10000, R(god)=F(a,god)*F(god,buy)*S(god)=10*5*3=150.
Since we pick the larger one as a result, then the final result for list(good, god) is good. 
